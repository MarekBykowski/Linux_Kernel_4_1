--- Evaluation of Parallelism in threaded tests ---

The key thing I try to address here is whether an assumption of summing up the
BWs from all the threads to achieve the total BW is valid. The short answer is
yes provided all the threads run in parallel. So do they run in parallel?

- kernel used is rt preempt 4.9

 'Linux axx-w024-a53 4.9.82-rt61 #152 SMP Wed Mar 20 07:02:36 CDT 2019 aarch64 aarch64 GNU/Linux'

- recall: LionFish GPDMA features 2x DMAs, each 2 channels = 4 channels. In promotion
  of the parallelism a number of kthreads were spawned from 4 (1
  thread_per_channel * 4 channels) through 64 (16 threads_per_channel * 4
  channels):

   option #1: 48 (sg_buffers) * 1 threads_per_channel * 4 channels = 192 buffers
   option #2: 24 (sg_buffers) * 2 threads_per_channel * 4 channels = 192 buffers
   option #3: 12 (sg_buffers) * 4 threads_per_channel * 4 channels = 192 buffers
   option #4: 6  (sg_buffers) * 6 threads_per_channel * 4 channels = 192 buffers
   option #5: 3  (sg_buffers) *16 threads_per_channel * 4 channels = 192 buffers

- threads distributions across the 32 cpus were mostly left to the sched though
  results with binding the threads to the isolated cpus (SMP affinity/cpu isolation)
  were also evaluated. When threads are created without sticking them to the particular
  cpu they happen to be migrated in the course of a single run,  eg. with 4x threads:

   # ps -eo psr,comm | grep dma[[:digit:]]
   cpu_no | thread_name
   20 dma0chan0-sg0
   17 dma0chan1-sg0
   19 dma1chan0-sg0
   18 dma1chan1-sg0

   and at some later:
   cpu_no | thread_name
   18 dma0chan0-sg0
   20 dma0chan1-sg0
   8 dma1chan0-sg0
   25 dma1chan1-sg0
   the threads got migrated to some other cpus.


- do the threads run in parallel?

  No, but let us analyze of how the "no" may affect the correctiveness of
  the results.

  #1 engine->lock (dma engine lock)
	 With 2x gpdma controllers we have 2x descriptor lists (probably it could
     get optimized into switching to 4x lists in the lsi-dma32.c driver). With
	 anything more than 2x threads (note we have at minimum 4x threads) threads
     contend to the access to the descriptors. The descriptors are protected
     by the spinlock (rt_spinlock which facilitates sleeping). With no spinning
	 (single thread) the lock acquire time is from 90 to 400 ns:

		# entries-in-buffer/entries-written: 40/40   #P:32
		#
		#                              _-----=> irqs-off
		#                             / _----=> need-resched
		#                            |/  _-----=> need-resched_lazy
		#                            || / _---=> hardirq/softirq
		#                            ||| / _--=> preempt-depth
		#                            |||| / _-=> preempt-lazy-depth
		#                            ||||| / _-=> migrate-disable
		#                            |||||| /    delay
		#           TASK-PID   CPU#  |||||||   TIMESTAMP  FUNCTION
		#              | |       |   |||||||      |         |
		   dma0chan1-sg0-4203  [004] dn.....  4017.113086: get_descriptor:  callled from gpdma_prep_sg spintime 265 ns
		   dma0chan1-sg0-4203  [004] dn.....  4017.113090: get_descriptor:  callled from gpdma_prep_sg spintime 90 ns
		   dma0chan1-sg0-4203  [004] d......  4017.113350: get_descriptor:  callled from gpdma_prep_sg spintime 379 ns
		   [...]
		   dma0chan1-sg0-4203  [004] d......  4017.114110: get_descriptor:  callled from gpdma_prep_sg spintime 98 ns


	 With a number of threads 4 > descriptors lists 2, threads must contend
	 though this doesn't seem a bottleneck in acquiring the descriptor (times
	 range the same):

		# entries-in-buffer/entries-written: 160/160   #P:32
		#
		#                              _-----=> irqs-off
		#                             / _----=> need-resched
		#                            |/  _-----=> need-resched_lazy
		#                            || / _---=> hardirq/softirq
		#                            ||| / _--=> preempt-depth
		#                            |||| / _-=> preempt-lazy-depth
		#                            ||||| / _-=> migrate-disable
		#                            |||||| /    delay
		#           TASK-PID   CPU#  |||||||   TIMESTAMP  FUNCTION
		#              | |       |   |||||||      |         |
		   dma0chan0-sg0-4133  [003] dn.....  3446.049052: get_descriptor:  callled from gpdma_prep_sg spintime 369 ns
		   dma1chan0-sg0-4135  [005] dn.....  3446.049148: get_descriptor:  callled from gpdma_prep_sg spintime 90 ns
		   dma0chan0-sg0-4133  [003] d......  3446.049315: get_descriptor:  callled from gpdma_prep_sg spintime 129 ns
		   [...]
		   dma0chan0-sg0-4133  [003] d......  3446.049318: get_descriptor:  callled from gpdma_prep_sg spintime 114 ns



  #2 vc.lock (virtual channel lock)

	 Throughout the copying the descriptor move through allocated, submitted,
	 issued, and completed queues. All the queues are being protected by vc.lock
     (lock per channel). So 1 through 4 threads don't compete (and by it satisfy
	 the simultaneous operation), 4 through don't. I haven't run any
	 instrumentation in here though infer from #1 that the results shouldn't expose
	 that the resource competition affects the total BW computation much.


- SMP affinity/cpu isolation imact on the total BW?

  To dedicate the cpus soley to the threads are to run I followed the below.
  Results shown towards the end do not show dedicating the cpus to dma threads
  bring 'any' benefits.

--- Isolating a core from the Linux Scheduler ---

Say, I want to isolate core7. Note the cores (another commonly used name
especially around the Linux kernel is cpu) are indexed 0 through
number_of_cpus-1. The steps are:
•	stop the system in Uboot prompt and run:
setenv bootargs 'root=/dev/nfs rw mem=2G console=ttyAMA0 ip=dhcp isolcpus=7 nohz_full=7 rcu_nocbs=7'
run bootcmd

•	mem=2G is safe enough though it should match the memory layout
configured/set in the ASE.
•	saveenv causes the bootargs variable to gets flashed to the Uboot env used at
the time (Uboot has 2x of them, primary and secondary). We should remember to
set it back after done with the time shared XLF
•	isolcpus=7 should offload cpu7 from any user-space processes. Can you
confirm this is only for the user-space processes?

To check if it went through we should have no user-space tasks running on the
cpu7 (we can check that through  /proc/sched_debug). /proc/sched_debug isn’t
enabled by default. To enable one can run make menuconfig, search for
sched_debug and add it in to the config)
•	rcu_nocbs=7 defines that cpu7 should run no RCU callbacks. For it we have to
have CONFIG_RCU_NOCB_CPU defined. By default XLF kernel doesn’t have it set.  It
is only becoming enabled after CONFIG_RCU_EXPERT gets enabled.

To check if rcu_nocbs takes effect… RCU callbacks of the cpu7 isolated should
get offloaded to rcuos/7 and rcuob/7 kthreads. I’m seeing them running on cpu0
(again from /proc/sched_debug):
rcuos/7    63       439.964522         3   120         0.000000         0.006731  0.000000 /
rcuob/7    64       439.966420         3   120         0.000000         0.007394  0.000000 /
•	nohz_full=7 takes effect only in the kernels with CONFIG_NO_HZ_FULL set. Our
kernel is running CONFIG_NO_HZ_IDLE. With NO_HZ_FULL and nohz_full=7 kernel
shouldn’t interrupt the cpu7 for any of the tasks to run.

The CONFIG_NO_HZ_FULL=y Kconfig option causes the kernel to avoid
sending scheduling-clock interrupts to CPUs with a single runnable task,
and such CPUs are said to be "adaptive-ticks CPUs".

By default, no CPU will be an adaptive-ticks CPU.  The "nohz_full="
boot parameter specifies the adaptive-ticks CPUs.  For example,
"nohz_full=1,6-8" says that CPUs 1, 6, 7, and 8 are to be adaptive-ticks
CPUs.

To check if it works first we should have no tick counts increasing on the cpu
isolated (/proc/interrupts) (More preciasely we should have a tick count per
second only when the core is running our app). With me arch_timer stopped firing
at the 285 count (below) for cpu7.

Excerpt from /proc/interrupts (cutting in cpu6 through 12)

         CPU6       CPU7       CPU8       CPU9       CPU10      CPU11      CPU12
 1:          0          0          0          0          0          0          0
0          0          0         GICv3  25 Level     vgic
2:          0          0          0          0          0          0          0
0          0          0         GICv3  29 Level     arch_timer
3:      3294        285       1734       2739       1429       2087       1578
1356    GICv3  30 Level     arch_timer

•	also a comment on commonly met “isolcpus=0-6 nohz_full=0-6 rcu_nocbs=0-6”
below. It doesn't reroute the IRQs away off cpu0. To achive that I think we
should look around IRQ affinity, part of the GIC driver routines.


- 4 threads_per_channel with and without binding threads to the particular cpus
  Option used 'option #1: 48 (sg_buffers) * 1 threads_per_channel * 4 channels = 192 buffers'

  #1 Not binding threads to any particular cpus

	Test took 57 sec. lmbench in the background no
	L3:l3_lock_to_l3_lock iter:10000 buf_size:6580 sg_buffers:48 threads_per_chan:1 => BW 26434 Mbps

	Test took 56 sec. lmbench in the background no
	L3:l3_lock_to_l3_lock iter:10000 buf_size:6592 sg_buffers:48 threads_per_chan:1 => BW 53186 Mbps

	Test took 58 sec. lmbench in the background no
	L3:l3_lock_to_l3_unlock iter:10000 buf_size:6580 sg_buffers:48 threads_per_chan:1 => BW 27228 Mbps

	Test took 56 sec. lmbench in the background no
	L3:l3_lock_to_l3_unlock iter:10000 buf_size:6592 sg_buffers:48 threads_per_chan:1 => BW 55489 Mbps

	Test took 59 sec. lmbench in the background no
	L3:l3_unlock_to_l3_unlock iter:10000 buf_size:6580 sg_buffers:48 threads_per_chan:1 => BW 26332 Mbps

	Test took 56 sec. lmbench in the background no
	L3:l3_unlock_to_l3_unlock iter:10000 buf_size:6592 sg_buffers:48 threads_per_chan:1 => BW 55332 Mbps

  #2 Binding threads to isolated cpus

	Test took 57 sec. lmbench in the background no
	L3:l3_lock_to_l3_lock iter:10000 buf_size:6580 sg_buffers:48 threads_per_chan:1 => BW 33006 Mbps

	Test took 56 sec. lmbench in the background no
	L3:l3_lock_to_l3_lock iter:10000 buf_size:6592 sg_buffers:48 threads_per_chan:1 => BW 54683 Mbps

	Test took 58 sec. lmbench in the background no
	L3:l3_lock_to_l3_unlock iter:10000 buf_size:6580 sg_buffers:48 threads_per_chan:1 => BW 26993 Mbps

	Test took 56 sec. lmbench in the background no
	L3:l3_lock_to_l3_unlock iter:10000 buf_size:6592 sg_buffers:48 threads_per_chan:1 => BW 55397 Mbps

	Test took 59 sec. lmbench in the background no
	L3:l3_unlock_to_l3_unlock iter:10000 buf_size:6580 sg_buffers:48 threads_per_chan:1 => BW 26663 Mbps

	Test took 56 sec. lmbench in the background no
	L3:l3_unlock_to_l3_unlock iter:10000 buf_size:6592 sg_buffers:48 threads_per_chan:1 => BW 53183 Mbps

  I don't see much difference which promotes the statement that the threads run
  in parallel. 
